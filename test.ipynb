{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f622f930",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a3dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd68e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04697b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6b77ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"20210930_q_a.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c5b503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick random question\n",
    "random_num = np.random.randint(0,len(data))\n",
    "question = data[\"Text\"][random_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce9ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer tex is pre-processed from the su text\n",
    "answer_text =  \"\"\"\n",
    "Yeah. Okay. So We come back.\n",
    "Good morning.\n",
    "Nice to be back\n",
    "Hello. How are you?\n",
    "I am very relaxed. Per Perez was asked them and the respite trip before. That was very nice as well. I can only recommend doing that in the north of Italy. The mountains are really nice and Paris is nice as well.\n",
    "So did you find any, uh, locks or something?\n",
    "Yeah, we even placed the luck. One of the bridges, useless metal\n",
    "Needs to be fixed\n",
    "To a bridge. Exactly until it, the weight takes it down.\n",
    "I think they're actually collecting them on a regular basis, but let's not go into deep because then the magic is  \n",
    "Yeah. Usually three or four years, they take them. I know that from at least from Salzburg, because that bridge had to be closed because at some point there were too many locks on it and it was in danger of collapsing.\n",
    "Oh yes, it can. You have no idea how\n",
    "There were about 14 or 16, tons of those locks attached like a regular pedestrian and it wasn't for that weight and the moment it was full with people, it started swinging too much and that's when they decided to close it down until they removed all the ducks. And so since then they are removing them on a regular basis. Definitely. I can guarantee there enough Romans So far for you, Somebody was yes, Luke. So how was the week so far? I would say\n",
    "It did not miss you too much\n",
    "Either. I'm very annoyed when I'm here\n",
    "Um, could you do me a favor type while you speaking?\n",
    "I don't actually. I just,\n",
    "Yeah, whatever you're doing with your fingers. Okay.\n",
    "Okay. From slack. So I go back and forth between,\n",
    "Okay.\n",
    "Okay. So I can continue to say this today. I finished the, um, the mother of the topic there. So for any position generates the kind of the doc that was discussed. So I just tried to improve it cause it was a bit immature generated.\n",
    "Good.\n",
    "I will take care of, well again, the web server.\n",
    "All right. Are you in factory today?\n",
    "Yes. Yes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa60bf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(question, answer_text)\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da54f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "num_seg_a = sep_index + 1\n",
    "\n",
    "num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "assert len(segment_ids) == len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339333c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                             token_type_ids=torch.tensor([segment_ids]), # The segment IDs to differentiate question from answer_text\n",
    "                             return_dict=True) \n",
    "\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1873d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_start = torch.argmax(start_scores)\n",
    "answer_end = torch.argmax(end_scores)\n",
    "\n",
    "answer = ' '.join(tokens[answer_start:answer_end+1])\n",
    "\n",
    "print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffdea31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed38d67f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
